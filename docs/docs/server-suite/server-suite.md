---
title: Jan Server Suite
slug: /server-suite
description: Built for Enterprise Deployments
keywords:
  [
    Jan AI,
    Jan,
    ChatGPT alternative,
    local AI,
    private AI,
    conversational AI,
    no-subscription fee,
    large language model,
  ]
---

# Jan Server Suite is for Teams

Jan Server Suite can be used as a professional backend to create, customize and run AIs at scale.

Self-host Jan anymore, from your homeservers to production-grade data centers.

:::warning

The server suite is actively under development and lacking documentation.
You can find the source code [here](https://github.com/janhq/jan/tree/dev/server) and [here](https://github.com/janhq/jan/blob/dev/docker-compose.yml).

It is free to use. Your feedback is appreciated üôè.

:::

## Own your AI. Own your data. Own your IP.

Over time, we expect more teams and organizations to turn to running their own AIs on-prem.

**Why?**

- Prevent shadow data
- Avoid vendor lock-in
- Keep your IP in house
- Uptime and support predictability
- Eliminate monthly API bills - use your existing hardware
- Full control over your AI - you can open it up and see what's going on

## Why Jan Server Suite

We built [Jan Desktop](/desktop) for our personal use. We're now building Server Suite, for our team & community, as a part of our [build in public](/how-we-work) ethos.

Our goal is to help teams, like ours, move past cobbling together demo apps to use AI at work. We should be able to customize and collaborate with AIs that are usable on a daily basis.

Check out [Server Suite](https://github.com/janhq/jan/tree/dev/server) if you need:

### Fast deployment

- **1 click deployment**. Immediately serve, customize, and scale models and assistants across your org. Scale your AI team so they can focus on the IP instead of fixing plumbing across every computer.
- **Scale across infrastructures**: on premise, with cloud providers, or as a hybrid deployment. Run Jan in completely air-gapped environments.
- **Optimized for datacenter-grade GPUs**: Can run on Nvidia, AMD Hardware, or even normal CPUs. Use TensorRT-LLM for more speedups on A6000s and above.

### Full customization

- Runs custom models or popular LLMs like Llama2, Mistral at production scale
- API that is fully OpenAI-compatible, i.e. can be a drop-in migration
- Powerful Agent framework to customize LLMs using RAG or Enterprise Data integrations.

:::tip

Not a Jan fan but convinced about local AI? No worries, here's a list of [awesome local ai](https://github.com/janhq/awesome-local-ai) alternatives that you can use in your team.

:::

## Features

The SDK and current implemention accomodate the following potential extensions.

### Admin console

Integrate SAML, OAUTH, OIDC

<!-- link to that page -->

### Identity access management

Grant roles, groups and general ACL

<!-- link to that page -->

### Audit compliance

Plug in Guardrails, LLMGuard, your custom rules engine and more

<!-- Link to that page -->

### Observability

Plug in Langfuse, Langsmith, Openllmetry and more

<!-- Link to this page -->

## Enterprise support SLA

Our core team and AI solutions partners are to help.

Email us at: `inquiries@jan.ai` for:

- Priority case routing
- Proactive case monitoring
- 24-hour support response
