# Models Spec v1
:::warning

Draft Specification: functionality has not been implemented yet. 

Feedback: [HackMD: Models Spec](https://hackmd.io/ulO3uB1AQCqLa5SAAMFOQw) 

:::

## Overview

Jan's Model API aims to be as similar as possible to [OpenAI's Models API](https://platform.openai.com/docs/api-reference/models), with additional methods for managing and running models locally. 

### Objectives

- Users can download, import and delete models  
- Users can use remote models (e.g. OpenAI, OpenRouter)
- Users can start/stop models and use them in a thread (or via Chat Completions API)
- User can configure default model parameters at the model level (to be overridden later at `chat/completions` or `assistant`/`thread` level)

## Design Principle
- Don't go for simplicity yet
- Underlying abstractions are changing very frequently (e.g. ggufv3)
- Provide a minimalist framework over the abstractions that takes care of coordination between tools
- Show direct system state for now

## KIVs to Model Spec v2
- OpenAI and Azure OpenAI
- Importing via URL
- Multiple Partitions

## Models folder structure
- Models in Jan are stored in the `/models` folder.
- Models are stored and organized by folders, which are atomic representations of a model for easy packaging and version control. 
```sh
/jan/    # Jan root folder
  /models/
    llama2-70b-q4_k_m/
        model-binary-1.gguf
        model.json
    mistral-7b-gguf-q3_k_l/
        model.json
        mistral-7b-q3-K-L.gguf
      mistral-7b-gguf-q8_k_m./
        model.json
        mistral-7b-q8_k_k.gguf
      random-model-q4_k_m/
        random-model-q4_k_m.bin
        random-model-q4_k_m.json # (autogenerated)
```

## Model Object
- Jan represents models as `json`-based Model Object files, known colloquially as `model.json`.
-Jan aims for rough equivalence with [OpenAI's Model Object](https://platform.openai.com/docs/api-reference/models/object) with additional properties to support local models.  
- Jan's models follow a `model.json` naming convention, and are built to be extremely lightweight, with the only mandatory field being a `source_url` to download the model binaries. 

### Types of Models

There are 3 types of models. 

- [x] Local model, yet-to-be downloaded (we have the URL)
- [x] Local model (downloaded)

## Examples
### Local Model

- Model has 1 binary `model-zephyr-7B.json`
- See [source](https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF/)

#### `model.json`
```json
"type": "model",
"version": "1",
"id": "zephyr-7b" // used in chat-completions model_name, matches folder name
"name": "Zephyr 7B"
"owned_by": "" // OpenAI compatibility
"created": 1231231 // unix timestamp
"description": "..." 
"state": enum[null, "downloading", "available"]
// KIV: remote: // Subsequent
// KIV: type: "llm" // For future where there are different types
"format": "ggufv3", // State format, rather than engine
"source_url": "https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF/blob/main/zephyr-7b-beta.Q4_K_M.gguf",
"settings" {
  "ctx_len": "2048",
  "ngl": "100",
  "embedding": "true",
  "n_parallel": "4",
  // KIV: "pre_prompt": "A chat between a curious user and an artificial intelligence",
  // KIV:"user_prompt": "USER: ",
  // KIV: "ai_prompt": "ASSISTANT: "
}
"parameters": {
  "temperature": "0.7",
  "token_limit": "2048",
  "top_k": "0",
  "top_p": "1",
  "stream": "true"
  },
  "metadata": {}
  "assets": [
      "file://.../zephyr-7b-q4_k_m.bin",
      "https://huggin"
  ]
```

### Deferred Download
```sh
models/
    mistral-7b/
        model.json
    hermes-7b/
        model.json
```
- Jan ships with a default model folders containing recommended models
- Only the Model Object `json` files are included
- Users must later explicitly download the model binaries

### Multiple model partitions

```sh
llava-ggml-Q5/
    model.json
    mmprj.bin
    model_q5.ggml
```

### Locally fine-tuned/ custom imported model

```sh
llama-70b-finetune/
    llama-70b-finetune-q5.json
    .bin
```

## Models API

| Method         | API Call                        | OpenAI-equivalent |
| -------------- | ------------------------------- | ----------------- |
| List Models    | GET /v1/models                  | true              |
| Get Model      | GET /v1/models/{model_id}       | true              |
| Delete Model   | DELETE /v1/models/{model_id}    | true              |
| Start Model    | PUT /v1/models/{model_id}/start | no                |
| Stop Model     | PUT /v1/models/{model_id}/start | no                |
| Download Model | POST /v1/models/                | no                |

## Importing Models

:::warning

- This has not been confirmed
- Jan should auto-detect and create folders automatically
- Jan's UI will allow users to rename folders and add metadata

:::

You can import a model by just dragging it into the `/models` folder, similar to Oobabooga. 

- Jan will detect and generate a corresponding `model.json` file based on model asset filename
- Jan will move it into its own `/model-id` folder once you define a `model-id` via the UI
- Jan will populate the model's `/model-id/model.json` as you add metadata through the UI

### Jan Model Importers extension

:::caution

- This is only an idea, has not been confirmed as part of spec

:::

Jan builds "importers" for users to seamlessly import models from a single URL. 

We currently only provide this for [TheBloke models on Huggingface](https://huggingface.co/TheBloke) (i.e. one of the patron saints of llama.cpp), but we plan to add more in the future. 

Currently, pasting a TheBloke Huggingface link in the Explore Models page will fire an importer, resulting in an: 

- Nicely-formatted model card
- Fully-annotated `model.json` file

### ADR
- `<model-id>.json`, i.e. the [Model Object](#model-object)
- Why multiple folders?
    - Model Partitions (e.g. Llava in the future)
- Why a folder and config file for each quantization?
    - Differently quantized models are completely different models
- Milestone -1st December: 
    - Catalogue of recommended models, anything else = mutate the filesystem 
- [@linh] Should we have an API to help quantize models? 
    - Could be a really cool feature to have (i.e. import from HF, quantize model, run on CPU)
- We should have a helper function to handle hardware compatibility
    - POST model/{model-id}/compatibility
- [louis] We are combining states & manifest
    - Need to think through